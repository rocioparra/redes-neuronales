{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8mBlkilaNC2M"
   },
   "outputs": [],
   "source": [
    "# Estos dos comandos evitan que haya que hacer reload cada vez que se modifica un paquete\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4XI-bramQQLk"
   },
   "source": [
    "# Ejercicio de clasificación de texto\n",
    "\n",
    "Naive Bayes es una técnica estadística que consiste en repetir el método anterior en problemas cuyos sucesos no son independientes, pero suponiendo independencia.\n",
    "A lo largo de este trabajo desarrollarán un modelo de Naive Bayes para el problema de clasificación de artículos periodístios.En este caso podemos estimar la probabilidad de ocurrencia de cada palabra según la categoría a la que pertenece el artículo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zJ7IlPHXEDEJ"
   },
   "source": [
    "## Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z8adA_beEXG7"
   },
   "source": [
    "El primer paso es obtener el dataset que vamos a utilizar. El dataset a utilizar es el de TwentyNewsGroup(TNG) que está disponible en sklearn.\n",
    "\n",
    "Se puede encontrar más información del dataset en la documentación de scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "AHaexjmQNQej",
    "outputId": "2dd3fd54-8454-4dde-8c08-d2b917679fdf"
   },
   "outputs": [],
   "source": [
    "#Loading the data set - training data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from os.path import isfile\n",
    "import pickle\n",
    "\n",
    "TT_FILE = 'twenty-train-nometadata.p'\n",
    "if isfile(TT_FILE):\n",
    "    twenty_train = pickle.load(open(TT_FILE, 'rb'))\n",
    "else:\n",
    "    twenty_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), shuffle=True)\n",
    "    pickle.dump(twenty_train, open(TT_FILE, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u65YieUTEgR3"
   },
   "source": [
    "El siguiente paso es analizar el contenido del dataset, como por ejemplo la cantidad de artículos, la cantidad de clases, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sx2_w9hbS9-D"
   },
   "source": [
    "Preguntas:\n",
    "\n",
    "1) ¿Cuántos articulos tiene el dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dmBRcNNbRvVu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twenty_train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) ¿Cuántas clases tiene el dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twenty_train.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) ¿Es un dataset balanceado?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El dataset no está balanceado\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "_, counts = np.unique(twenty_train.target, return_counts=True)\n",
    "if len(set(counts)) == 1:\n",
    "    print('El dataset está balanceado')\n",
    "else:    \n",
    "    print('El dataset no está balanceado')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) ¿Cuál es la probabilidad a priori de la clase 5? A que corresponde esta clase?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La clase 5 (comp.windows.x) tiene probabilidad a priori 0.052\n"
     ]
    }
   ],
   "source": [
    "priori5 = counts[5]/sum(counts)\n",
    "print(f'La clase 5 ({twenty_train.target_names[5]}) tiene probabilidad a priori {priori5:.2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) ¿Cuál es la clase con mayor probabilidad a priori?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_class = np.argmax(counts)\n",
    "print(f'La clase {max_class} ({twenty_train.target_names[max_class]}) tiene la máxima probabilidad a priori, {counts[max_class]/sum(counts):.2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84hQvk-UHX1V"
   },
   "source": [
    "## Preprocesamiento\n",
    "\n",
    "Para facilitar la comprensión de los algoritmos de preprocesamiento, se aplican primero a un solo artículo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l0zxZSOsFXpl"
   },
   "source": [
    "\n",
    "Mas info en:\n",
    "http://text-processing.com/demo/stem/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = twenty_train.data[0]\n",
    "article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-geItZPyUWEO"
   },
   "source": [
    "- **Tokenization (nltk):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "tok = word_tokenize(article)\n",
    "tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pksduBGKFMxt"
   },
   "source": [
    "\n",
    "- **Lemmatization (nltk):**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lem=[lemmatizer.lemmatize(x,pos='v') for x in tok]\n",
    "lem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-zxqqFYAFOsN"
   },
   "source": [
    "- **Stop Words (nltk):**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop = [x for x in lem if x not in stopwords.words('english')]\n",
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UjKQKWLGFQRl"
   },
   "source": [
    "- **Stemming (nltk):**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stem = [stemmer.stem(x) for x in stop]\n",
    "stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HNfMRbqmFSIR"
   },
   "source": [
    "- **Filtrado de palabras:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [x for x in stem if x.isalpha()]\n",
    "alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yOzGVenvIESr"
   },
   "source": [
    "### Preprocesamiento completo\n",
    "\n",
    "Utilizar o no cada uno de los métodos vistos es una decisión que dependerá del caso particular de aplicación. Para este ejercicio vamos a considerar las siguientes combinaciones:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenización\n",
    "- Tokenización, Lematización, Stemming.\n",
    "- Tokenización, Stop Words.\n",
    "- Tokenización, Lematización, Stop Words, Stemming.\n",
    "- Tokenización, Lematización, Stop Words, Stemming, Filtrado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_article(article, filts):\n",
    "    filts = filts.split()\n",
    "\n",
    "    if 'lem' in filts:\n",
    "        article = [lemmatizer.lemmatize(x,pos='v') for x in article]\n",
    "\n",
    "    if 'stop' in filts:\n",
    "        article = [x for x in article if x not in stopwords.words('english')]\n",
    "\n",
    "    if 'stem' in filts:\n",
    "        article = [stemmer.stem(x) for x in article]\n",
    "\n",
    "    if 'filt' in filts:\n",
    "        article = [x for x in article if x.isalpha()]\n",
    "    \n",
    "    return article\n",
    "\n",
    "\n",
    "def filter_articles(name, articles, filts):\n",
    "    \n",
    "    filename = f'{name}-{preproc}.p'\n",
    "    if os.path.isfile(filename):\n",
    "        with open (filename, 'rb') as fp:\n",
    "            filtered_articles = pickle.load(fp)\n",
    "    \n",
    "    else:\n",
    "        filtered_articles = []\n",
    "        for data in articles:\n",
    "            tok = word_tokenize(data)\n",
    "            filtered_articles.append(filter_article(tok, filts))\n",
    "        \n",
    "        with open(filename, 'wb') as fp:\n",
    "            pickle.dump(filtered_articles, fp)\n",
    "            \n",
    "    return filtered_articles\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocesamiento: tok\n",
      "Longitud del vocabulario: 161698\n",
      "\n",
      "Preprocesamiento: tok lem stem\n",
      "Longitud del vocabulario: 126903\n",
      "\n",
      "Preprocesamiento: tok stop\n",
      "Longitud del vocabulario: 161533\n",
      "\n",
      "Preprocesamiento: tok lem stop stem\n",
      "Longitud del vocabulario: 126872\n",
      "\n",
      "Preprocesamiento: tok lem stop stem filt\n",
      "Longitud del vocabulario: 45315\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ans_fmt = \"\"\"Preprocesamiento: {preproc}\n",
    "Longitud del vocabulario: {vocab_len}\n",
    "\"\"\"\n",
    "preprocessing = ['tok', 'tok lem stem', 'tok stop', 'tok lem stop stem', 'tok lem stop stem filt']\n",
    "\n",
    "    \n",
    "for preproc in preprocessing:\n",
    "    filtered_articles = filter_articles('train-nometadata', twenty_train.data, preproc)\n",
    "    vocab = set([word for article in filtered_articles for word in article])\n",
    "    print(ans_fmt.format(preproc=preproc, vocab_len=len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wP3n8ZwFc7_D"
   },
   "source": [
    "Preguntas\n",
    "\n",
    "- Cómo cambia el tamaño del vocabulario al agregar Lematización y Stemming?\n",
    "- Cómo cambia el tamaño del vocabulario al Stop Words?\n",
    "- Analice muy brevemente ventajas y desventajas del tamaño del dataset en cada caso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c-R-LPRoIV0E"
   },
   "source": [
    "## Vectorización de texto\n",
    "\n",
    "- **Obtención del vocabulario y obtención de la probabilidad**\n",
    "\n",
    "Como se vió en clase, los vectorizadores cuentan con dos parámetros de ajuste.\n",
    "\n",
    "- max_df: le asignamos una maxima frecuencia de aparición, eliminando las palabras comunes que no aportan información.\n",
    "\n",
    "- min_df: le asignamos la minima cantidad de veces que tiene que aparecer una palabra.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_file_fmt = '{name}-joined-{filts}.p'\n",
    "for preproc in preprocessing:\n",
    "    filename = joined_file_fmt.format(name='train-nometadata', filts=preproc)\n",
    "\n",
    "    if not os.path.isfile(filename):\n",
    "        with open(f'train-nometadata-{preproc}.p', 'rb') as fp:\n",
    "            articles = pickle.load(fp)\n",
    "            for i in range(len(articles)):\n",
    "                articles[i] = ' '.join(articles[i])\n",
    "            \n",
    "            with open(filename, 'wb') as fp_joined:\n",
    "                pickle.dump(articles, fp_joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtered_joined_articles(name, filts):\n",
    "    filename = joined_file_fmt.format(name=name, filts=filts)\n",
    "    with open(filename, 'rb') as fp:\n",
    "            articles = pickle.load(fp)\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C74USIEyS3B7"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "import pandas as pd\n",
    "\n",
    "OUT_FILE = 'out-nometadata.csv'\n",
    "\n",
    "vectorizers = [CountVectorizer, TfidfVectorizer]\n",
    "classifiers = [MultinomialNB, BernoulliNB]\n",
    "max_dfs = [0.50, 0.75, 0.80, 0.90, 0.95]\n",
    "min_dfs = [int((round(len(twenty_train.data)*n/100))) for n in [0.01, 0.1, 0.5, 1]]\n",
    "\n",
    "results = []\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "total_its = len(vectorizers)*len(classifiers)*len(min_dfs)*len(max_dfs)*len(preprocessing)\n",
    "i = 1\n",
    "\n",
    "if os.path.isfile(OUT_FILE):\n",
    "    results = pd.read_csv(OUT_FILE)\n",
    "else:\n",
    "    results = []\n",
    "\n",
    "    for filts in preprocessing:\n",
    "        data = get_filtered_joined_articles(name='train-nometadata', filts=filts) \n",
    "        \n",
    "        for max_df in max_dfs:\n",
    "            for min_df in min_dfs:\n",
    "                \n",
    "                for Vectorizer in vectorizers:\n",
    "                    count_vect = Vectorizer(max_df=max_df, min_df=min_df)\n",
    "                    raw_data = count_vect.fit_transform(data) \n",
    "\n",
    "                    for Classifier in classifiers:\n",
    "                        print(f'Computing: F={filts}, M={max_df}, m={min_df}, V={Vectorizer.__name__}, C={Classifier.__name__} ({i}/{total_its})...')\n",
    "                        i += 1\n",
    "                        \n",
    "                        clf = Classifier()\n",
    "                        clf.fit(raw_data, twenty_train.target)\n",
    "                        score = clf.score(raw_data, twenty_train.target)\n",
    "\n",
    "                        results.append({\n",
    "                            'max_df': max_df,\n",
    "                            'min_df': min_df,\n",
    "                            'filts': filts,\n",
    "                            'vectorizer': Vectorizer.__name__,\n",
    "                            'model': Classifier.__name__,\n",
    "                            'score': score\n",
    "                        })\n",
    "                        \n",
    "                        print(f'Score: {score:.5%}')\n",
    "                        print(50 * '-')\n",
    "    results = pd.DataFrame(results)\n",
    "    results.to_csv(OUT_FILE, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3iqIoNyTTYVc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_df</th>\n",
       "      <th>min_df</th>\n",
       "      <th>filts</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>model</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>tok lem stop stem filt</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.955895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     max_df  min_df                   filts       vectorizer          model  \\\n",
       "320     0.5       1  tok lem stop stem filt  CountVectorizer  MultinomialNB   \n",
       "\n",
       "        score  \n",
       "320  0.955895  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best = results['score'].max()\n",
    "best_filt = results['score']==best\n",
    "best_hypers = results[best_filt]\n",
    "best_hypers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_df</th>\n",
       "      <th>min_df</th>\n",
       "      <th>filts</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>model</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0.50</td>\n",
       "      <td>11</td>\n",
       "      <td>tok stop</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.858494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>0.50</td>\n",
       "      <td>11</td>\n",
       "      <td>tok stop</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.858494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>0.90</td>\n",
       "      <td>11</td>\n",
       "      <td>tok stop</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.854870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>0.90</td>\n",
       "      <td>11</td>\n",
       "      <td>tok stop</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.854870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>0.95</td>\n",
       "      <td>11</td>\n",
       "      <td>tok stop</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.854870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>0.95</td>\n",
       "      <td>11</td>\n",
       "      <td>tok stop</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.854870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>0.75</td>\n",
       "      <td>11</td>\n",
       "      <td>tok stop</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.854870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>0.75</td>\n",
       "      <td>11</td>\n",
       "      <td>tok stop</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.854870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.80</td>\n",
       "      <td>11</td>\n",
       "      <td>tok stop</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.854870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.80</td>\n",
       "      <td>11</td>\n",
       "      <td>tok stop</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.854870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>0.50</td>\n",
       "      <td>11</td>\n",
       "      <td>tok lem stop stem</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.853191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>0.50</td>\n",
       "      <td>11</td>\n",
       "      <td>tok lem stop stem</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.853191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>0.75</td>\n",
       "      <td>11</td>\n",
       "      <td>tok lem stop stem</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.851953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>0.80</td>\n",
       "      <td>11</td>\n",
       "      <td>tok lem stop stem</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.851953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>0.95</td>\n",
       "      <td>11</td>\n",
       "      <td>tok lem stop stem</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.851953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>0.95</td>\n",
       "      <td>11</td>\n",
       "      <td>tok lem stop stem</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.851953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>0.75</td>\n",
       "      <td>11</td>\n",
       "      <td>tok lem stop stem</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.851953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>0.90</td>\n",
       "      <td>11</td>\n",
       "      <td>tok lem stop stem</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.851953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>0.80</td>\n",
       "      <td>11</td>\n",
       "      <td>tok lem stop stem</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.851953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>0.90</td>\n",
       "      <td>11</td>\n",
       "      <td>tok lem stop stem</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.851953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.50</td>\n",
       "      <td>11</td>\n",
       "      <td>tok</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.841789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.50</td>\n",
       "      <td>11</td>\n",
       "      <td>tok</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.841789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.50</td>\n",
       "      <td>11</td>\n",
       "      <td>tok lem stem</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.838165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.50</td>\n",
       "      <td>11</td>\n",
       "      <td>tok lem stem</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.838165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.75</td>\n",
       "      <td>11</td>\n",
       "      <td>tok</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.836397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.75</td>\n",
       "      <td>11</td>\n",
       "      <td>tok</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.836397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.80</td>\n",
       "      <td>11</td>\n",
       "      <td>tok</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.834541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.80</td>\n",
       "      <td>11</td>\n",
       "      <td>tok</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.834541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.75</td>\n",
       "      <td>11</td>\n",
       "      <td>tok lem stem</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.833834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.80</td>\n",
       "      <td>11</td>\n",
       "      <td>tok lem stem</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.833834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.80</td>\n",
       "      <td>11</td>\n",
       "      <td>tok lem stem</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.833834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.75</td>\n",
       "      <td>11</td>\n",
       "      <td>tok lem stem</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.833834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.90</td>\n",
       "      <td>11</td>\n",
       "      <td>tok</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.833392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.90</td>\n",
       "      <td>11</td>\n",
       "      <td>tok</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.833392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.95</td>\n",
       "      <td>11</td>\n",
       "      <td>tok</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.833215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.95</td>\n",
       "      <td>11</td>\n",
       "      <td>tok</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.833215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0.90</td>\n",
       "      <td>11</td>\n",
       "      <td>tok lem stem</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.832508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0.90</td>\n",
       "      <td>11</td>\n",
       "      <td>tok lem stem</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.832508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.95</td>\n",
       "      <td>11</td>\n",
       "      <td>tok lem stem</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.831801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0.95</td>\n",
       "      <td>11</td>\n",
       "      <td>tok lem stem</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.831801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>0.50</td>\n",
       "      <td>11</td>\n",
       "      <td>tok lem stop stem filt</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.829326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>0.50</td>\n",
       "      <td>11</td>\n",
       "      <td>tok lem stop stem filt</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.829326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>0.90</td>\n",
       "      <td>11</td>\n",
       "      <td>tok lem stop stem filt</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.828089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>0.75</td>\n",
       "      <td>11</td>\n",
       "      <td>tok lem stop stem filt</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.828089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>0.90</td>\n",
       "      <td>11</td>\n",
       "      <td>tok lem stop stem filt</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.828089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>0.80</td>\n",
       "      <td>11</td>\n",
       "      <td>tok lem stop stem filt</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.828089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>0.80</td>\n",
       "      <td>11</td>\n",
       "      <td>tok lem stop stem filt</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.828089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>0.95</td>\n",
       "      <td>11</td>\n",
       "      <td>tok lem stop stem filt</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.828089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>0.75</td>\n",
       "      <td>11</td>\n",
       "      <td>tok lem stop stem filt</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.828089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>0.95</td>\n",
       "      <td>11</td>\n",
       "      <td>tok lem stop stem filt</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.828089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "      <td>tok lem stop stem filt</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.827382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "      <td>tok lem stop stem filt</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.827382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>0.80</td>\n",
       "      <td>1</td>\n",
       "      <td>tok lem stop stem filt</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.825614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>0.80</td>\n",
       "      <td>1</td>\n",
       "      <td>tok lem stop stem filt</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.825614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>0.75</td>\n",
       "      <td>1</td>\n",
       "      <td>tok lem stop stem filt</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.825614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>0.75</td>\n",
       "      <td>1</td>\n",
       "      <td>tok lem stop stem filt</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.825614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>0.90</td>\n",
       "      <td>1</td>\n",
       "      <td>tok lem stop stem filt</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.825614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>0.95</td>\n",
       "      <td>1</td>\n",
       "      <td>tok lem stop stem filt</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.825614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>0.95</td>\n",
       "      <td>1</td>\n",
       "      <td>tok lem stop stem filt</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.825614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>0.90</td>\n",
       "      <td>1</td>\n",
       "      <td>tok lem stop stem filt</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.825614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     max_df  min_df                   filts       vectorizer        model  \\\n",
       "165    0.50      11                tok stop  CountVectorizer  BernoulliNB   \n",
       "167    0.50      11                tok stop  TfidfVectorizer  BernoulliNB   \n",
       "215    0.90      11                tok stop  TfidfVectorizer  BernoulliNB   \n",
       "213    0.90      11                tok stop  CountVectorizer  BernoulliNB   \n",
       "231    0.95      11                tok stop  TfidfVectorizer  BernoulliNB   \n",
       "229    0.95      11                tok stop  CountVectorizer  BernoulliNB   \n",
       "181    0.75      11                tok stop  CountVectorizer  BernoulliNB   \n",
       "183    0.75      11                tok stop  TfidfVectorizer  BernoulliNB   \n",
       "197    0.80      11                tok stop  CountVectorizer  BernoulliNB   \n",
       "199    0.80      11                tok stop  TfidfVectorizer  BernoulliNB   \n",
       "245    0.50      11       tok lem stop stem  CountVectorizer  BernoulliNB   \n",
       "247    0.50      11       tok lem stop stem  TfidfVectorizer  BernoulliNB   \n",
       "261    0.75      11       tok lem stop stem  CountVectorizer  BernoulliNB   \n",
       "277    0.80      11       tok lem stop stem  CountVectorizer  BernoulliNB   \n",
       "309    0.95      11       tok lem stop stem  CountVectorizer  BernoulliNB   \n",
       "311    0.95      11       tok lem stop stem  TfidfVectorizer  BernoulliNB   \n",
       "263    0.75      11       tok lem stop stem  TfidfVectorizer  BernoulliNB   \n",
       "293    0.90      11       tok lem stop stem  CountVectorizer  BernoulliNB   \n",
       "279    0.80      11       tok lem stop stem  TfidfVectorizer  BernoulliNB   \n",
       "295    0.90      11       tok lem stop stem  TfidfVectorizer  BernoulliNB   \n",
       "5      0.50      11                     tok  CountVectorizer  BernoulliNB   \n",
       "7      0.50      11                     tok  TfidfVectorizer  BernoulliNB   \n",
       "85     0.50      11            tok lem stem  CountVectorizer  BernoulliNB   \n",
       "87     0.50      11            tok lem stem  TfidfVectorizer  BernoulliNB   \n",
       "23     0.75      11                     tok  TfidfVectorizer  BernoulliNB   \n",
       "21     0.75      11                     tok  CountVectorizer  BernoulliNB   \n",
       "37     0.80      11                     tok  CountVectorizer  BernoulliNB   \n",
       "39     0.80      11                     tok  TfidfVectorizer  BernoulliNB   \n",
       "101    0.75      11            tok lem stem  CountVectorizer  BernoulliNB   \n",
       "117    0.80      11            tok lem stem  CountVectorizer  BernoulliNB   \n",
       "119    0.80      11            tok lem stem  TfidfVectorizer  BernoulliNB   \n",
       "103    0.75      11            tok lem stem  TfidfVectorizer  BernoulliNB   \n",
       "55     0.90      11                     tok  TfidfVectorizer  BernoulliNB   \n",
       "53     0.90      11                     tok  CountVectorizer  BernoulliNB   \n",
       "69     0.95      11                     tok  CountVectorizer  BernoulliNB   \n",
       "71     0.95      11                     tok  TfidfVectorizer  BernoulliNB   \n",
       "133    0.90      11            tok lem stem  CountVectorizer  BernoulliNB   \n",
       "135    0.90      11            tok lem stem  TfidfVectorizer  BernoulliNB   \n",
       "149    0.95      11            tok lem stem  CountVectorizer  BernoulliNB   \n",
       "151    0.95      11            tok lem stem  TfidfVectorizer  BernoulliNB   \n",
       "325    0.50      11  tok lem stop stem filt  CountVectorizer  BernoulliNB   \n",
       "327    0.50      11  tok lem stop stem filt  TfidfVectorizer  BernoulliNB   \n",
       "373    0.90      11  tok lem stop stem filt  CountVectorizer  BernoulliNB   \n",
       "341    0.75      11  tok lem stop stem filt  CountVectorizer  BernoulliNB   \n",
       "375    0.90      11  tok lem stop stem filt  TfidfVectorizer  BernoulliNB   \n",
       "359    0.80      11  tok lem stop stem filt  TfidfVectorizer  BernoulliNB   \n",
       "357    0.80      11  tok lem stop stem filt  CountVectorizer  BernoulliNB   \n",
       "389    0.95      11  tok lem stop stem filt  CountVectorizer  BernoulliNB   \n",
       "343    0.75      11  tok lem stop stem filt  TfidfVectorizer  BernoulliNB   \n",
       "391    0.95      11  tok lem stop stem filt  TfidfVectorizer  BernoulliNB   \n",
       "321    0.50       1  tok lem stop stem filt  CountVectorizer  BernoulliNB   \n",
       "323    0.50       1  tok lem stop stem filt  TfidfVectorizer  BernoulliNB   \n",
       "355    0.80       1  tok lem stop stem filt  TfidfVectorizer  BernoulliNB   \n",
       "353    0.80       1  tok lem stop stem filt  CountVectorizer  BernoulliNB   \n",
       "337    0.75       1  tok lem stop stem filt  CountVectorizer  BernoulliNB   \n",
       "339    0.75       1  tok lem stop stem filt  TfidfVectorizer  BernoulliNB   \n",
       "369    0.90       1  tok lem stop stem filt  CountVectorizer  BernoulliNB   \n",
       "387    0.95       1  tok lem stop stem filt  TfidfVectorizer  BernoulliNB   \n",
       "385    0.95       1  tok lem stop stem filt  CountVectorizer  BernoulliNB   \n",
       "371    0.90       1  tok lem stop stem filt  TfidfVectorizer  BernoulliNB   \n",
       "\n",
       "        score  \n",
       "165  0.858494  \n",
       "167  0.858494  \n",
       "215  0.854870  \n",
       "213  0.854870  \n",
       "231  0.854870  \n",
       "229  0.854870  \n",
       "181  0.854870  \n",
       "183  0.854870  \n",
       "197  0.854870  \n",
       "199  0.854870  \n",
       "245  0.853191  \n",
       "247  0.853191  \n",
       "261  0.851953  \n",
       "277  0.851953  \n",
       "309  0.851953  \n",
       "311  0.851953  \n",
       "263  0.851953  \n",
       "293  0.851953  \n",
       "279  0.851953  \n",
       "295  0.851953  \n",
       "5    0.841789  \n",
       "7    0.841789  \n",
       "85   0.838165  \n",
       "87   0.838165  \n",
       "23   0.836397  \n",
       "21   0.836397  \n",
       "37   0.834541  \n",
       "39   0.834541  \n",
       "101  0.833834  \n",
       "117  0.833834  \n",
       "119  0.833834  \n",
       "103  0.833834  \n",
       "55   0.833392  \n",
       "53   0.833392  \n",
       "69   0.833215  \n",
       "71   0.833215  \n",
       "133  0.832508  \n",
       "135  0.832508  \n",
       "149  0.831801  \n",
       "151  0.831801  \n",
       "325  0.829326  \n",
       "327  0.829326  \n",
       "373  0.828089  \n",
       "341  0.828089  \n",
       "375  0.828089  \n",
       "359  0.828089  \n",
       "357  0.828089  \n",
       "389  0.828089  \n",
       "343  0.828089  \n",
       "391  0.828089  \n",
       "321  0.827382  \n",
       "323  0.827382  \n",
       "355  0.825614  \n",
       "353  0.825614  \n",
       "337  0.825614  \n",
       "339  0.825614  \n",
       "369  0.825614  \n",
       "387  0.825614  \n",
       "385  0.825614  \n",
       "371  0.825614  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[results.model=='BernoulliNB'].sort_values(by=['score'], ascending=False).head(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G7ob-IgsMRS7"
   },
   "source": [
    "## Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a4bVz9QwMupE"
   },
   "source": [
    "Primero deben separar correctamente el dataset para hacer validación del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5S9rAz2tNvkw"
   },
   "source": [
    "Y luego deben entrenar el modelo de NaiveBayes con el dataset de train.\n",
    "\n",
    "Deben utilizar un modelo de NaiveBayes Multinomial y de Bernoulli. Ambos modelos estan disponibles en sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mDUWaF5_VB4K"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Completar para cada caso según corresponda\n",
    "clf = MultinomialNB()\n",
    "# clf = BernoulliNB()\n",
    "clf.fit(raw_data, twenty_train.target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "692nzR2JOV1F"
   },
   "source": [
    "Finalmente comprobar el accuracy en train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vbidxDQ7VPCb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "porc=clf.score(raw_data, twenty_train.target)\n",
    "print(\"El porcentaje de artículos clasificados correctamente es: {:.2f}%\".format(porc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GPPo4Fr-eSON"
   },
   "source": [
    "Preguntas\n",
    "\n",
    "- Con que combinación de preprocesamiento obtuvo los mejores resultados? Explique por qué cree que fue así.\n",
    "\n",
    "- Con que modelo obtuvo los mejores resultados? Explique por qué cree que fue así."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mOGoiDhGe4MH"
   },
   "source": [
    "## Performance de los modelos\n",
    "\n",
    "En el caso anterior, para medir la cantidad de artículos clasiicados correctamente se utilizó el mismo subconjunto del dataset que se utilizó para entrenar.\n",
    "\n",
    "Esta medida no es una medida del todo útil, ya que lo que interesa de un clasificador es su capacidad de clasificación de datos que no fueron utilizados para entrenar. Es por eso que se pide, para el clasificador entrenado con el subconjunto de training, cual es el porcentaje de artículos del subconjunto de testing clasificados correctamente. Comparar con el porcentaje anterior y explicar las diferencias.\n",
    "\n",
    "Finalmente deben observar las diferencias y extraer conclusiones en base al accuracy obtenido, el preprocesamiento y vectorización utilizado y el modelo, para cada combinación de posibilidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fsnhfAiwg-A-"
   },
   "outputs": [],
   "source": [
    "# Solución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UpphW8cAVcrX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "porc=sum(np.array(clf.predict(X_test.toarray()))==np.array(Y_test))/len(Y_test)*100\n",
    "print(\"El porcentaje de artículos clasificados correctamente es: {:.2f}%\".format(porc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iOf-BcvpeqKp"
   },
   "source": [
    "Preguntas\n",
    "\n",
    "- El accuracy en el dataset de test es mayor o menor que en train? Explique por qué."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ejercicio_Clasificación_Texto.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
