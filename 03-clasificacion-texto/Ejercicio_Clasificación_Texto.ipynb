{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8mBlkilaNC2M"
   },
   "outputs": [],
   "source": [
    "# Estos dos comandos evitan que haya que hacer reload cada vez que se modifica un paquete\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4XI-bramQQLk"
   },
   "source": [
    "# Ejercicio de clasificación de texto\n",
    "\n",
    "Naive Bayes es una técnica estadística que consiste en repetir el método anterior en problemas cuyos sucesos no son independientes, pero suponiendo independencia.\n",
    "A lo largo de este trabajo desarrollarán un modelo de Naive Bayes para el problema de clasificación de artículos periodístios.En este caso podemos estimar la probabilidad de ocurrencia de cada palabra según la categoría a la que pertenece el artículo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zJ7IlPHXEDEJ"
   },
   "source": [
    "## Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z8adA_beEXG7"
   },
   "source": [
    "El primer paso es obtener el dataset que vamos a utilizar. El dataset a utilizar es el de TwentyNewsGroup(TNG) que está disponible en sklearn.\n",
    "\n",
    "Se puede encontrar más información del dataset en la documentación de scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "AHaexjmQNQej",
    "outputId": "2dd3fd54-8454-4dde-8c08-d2b917679fdf"
   },
   "outputs": [],
   "source": [
    "#Loading the data set - training data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from os.path import isfile\n",
    "import pickle\n",
    "\n",
    "TT_FILE = 'twenty-train.p'\n",
    "if isfile(TT_FILE):\n",
    "    twenty_train = pickle.load(open(TT_FILE, 'rb'))\n",
    "else:\n",
    "    twenty_train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "    pickle.dump(twenty_train, open(TT_FILE, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u65YieUTEgR3"
   },
   "source": [
    "El siguiente paso es analizar el contenido del dataset, como por ejemplo la cantidad de artículos, la cantidad de clases, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sx2_w9hbS9-D"
   },
   "source": [
    "Preguntas:\n",
    "\n",
    "1) ¿Cuántos articulos tiene el dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dmBRcNNbRvVu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twenty_train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) ¿Cuántas clases tiene el dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twenty_train.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) ¿Es un dataset balanceado?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El dataset no está balanceado\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "_, counts = np.unique(twenty_train.target, return_counts=True)\n",
    "if len(set(counts)) == 1:\n",
    "    print('El dataset está balanceado')\n",
    "else:    \n",
    "    print('El dataset no está balanceado')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) ¿Cuál es la probabilidad a priori de la clase 5? A que corresponde esta clase?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La clase 5 (comp.windows.x) tiene probabilidad a priori 0.052\n"
     ]
    }
   ],
   "source": [
    "priori5 = counts[5]/sum(counts)\n",
    "print(f'La clase 5 ({twenty_train.target_names[5]}) tiene probabilidad a priori {priori5:.2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) ¿Cuál es la clase con mayor probabilidad a priori?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La clase 10 (rec.sport.hockey) tiene la máxima probabilidad a priori, 0.053\n"
     ]
    }
   ],
   "source": [
    "max_class = np.argmax(counts)\n",
    "print(f'La clase {max_class} ({twenty_train.target_names[max_class]}) tiene la máxima probabilidad a priori, {counts[max_class]/sum(counts):.2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84hQvk-UHX1V"
   },
   "source": [
    "## Preprocesamiento\n",
    "\n",
    "Para facilitar la comprensión de los algoritmos de preprocesamiento, se aplican primero a un solo artículo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l0zxZSOsFXpl"
   },
   "source": [
    "\n",
    "Mas info en:\n",
    "http://text-processing.com/demo/stem/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article = twenty_train.data[0]\n",
    "article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-geItZPyUWEO"
   },
   "source": [
    "- **Tokenization (nltk):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rochi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['From',\n",
       " ':',\n",
       " 'lerxst',\n",
       " '@',\n",
       " 'wam.umd.edu',\n",
       " '(',\n",
       " 'where',\n",
       " \"'s\",\n",
       " 'my',\n",
       " 'thing',\n",
       " ')',\n",
       " 'Subject',\n",
       " ':',\n",
       " 'WHAT',\n",
       " 'car',\n",
       " 'is',\n",
       " 'this',\n",
       " '!',\n",
       " '?',\n",
       " 'Nntp-Posting-Host',\n",
       " ':',\n",
       " 'rac3.wam.umd.edu',\n",
       " 'Organization',\n",
       " ':',\n",
       " 'University',\n",
       " 'of',\n",
       " 'Maryland',\n",
       " ',',\n",
       " 'College',\n",
       " 'Park',\n",
       " 'Lines',\n",
       " ':',\n",
       " '15',\n",
       " 'I',\n",
       " 'was',\n",
       " 'wondering',\n",
       " 'if',\n",
       " 'anyone',\n",
       " 'out',\n",
       " 'there',\n",
       " 'could',\n",
       " 'enlighten',\n",
       " 'me',\n",
       " 'on',\n",
       " 'this',\n",
       " 'car',\n",
       " 'I',\n",
       " 'saw',\n",
       " 'the',\n",
       " 'other',\n",
       " 'day',\n",
       " '.',\n",
       " 'It',\n",
       " 'was',\n",
       " 'a',\n",
       " '2-door',\n",
       " 'sports',\n",
       " 'car',\n",
       " ',',\n",
       " 'looked',\n",
       " 'to',\n",
       " 'be',\n",
       " 'from',\n",
       " 'the',\n",
       " 'late',\n",
       " '60s/',\n",
       " 'early',\n",
       " '70s',\n",
       " '.',\n",
       " 'It',\n",
       " 'was',\n",
       " 'called',\n",
       " 'a',\n",
       " 'Bricklin',\n",
       " '.',\n",
       " 'The',\n",
       " 'doors',\n",
       " 'were',\n",
       " 'really',\n",
       " 'small',\n",
       " '.',\n",
       " 'In',\n",
       " 'addition',\n",
       " ',',\n",
       " 'the',\n",
       " 'front',\n",
       " 'bumper',\n",
       " 'was',\n",
       " 'separate',\n",
       " 'from',\n",
       " 'the',\n",
       " 'rest',\n",
       " 'of',\n",
       " 'the',\n",
       " 'body',\n",
       " '.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'all',\n",
       " 'I',\n",
       " 'know',\n",
       " '.',\n",
       " 'If',\n",
       " 'anyone',\n",
       " 'can',\n",
       " 'tellme',\n",
       " 'a',\n",
       " 'model',\n",
       " 'name',\n",
       " ',',\n",
       " 'engine',\n",
       " 'specs',\n",
       " ',',\n",
       " 'years',\n",
       " 'of',\n",
       " 'production',\n",
       " ',',\n",
       " 'where',\n",
       " 'this',\n",
       " 'car',\n",
       " 'is',\n",
       " 'made',\n",
       " ',',\n",
       " 'history',\n",
       " ',',\n",
       " 'or',\n",
       " 'whatever',\n",
       " 'info',\n",
       " 'you',\n",
       " 'have',\n",
       " 'on',\n",
       " 'this',\n",
       " 'funky',\n",
       " 'looking',\n",
       " 'car',\n",
       " ',',\n",
       " 'please',\n",
       " 'e-mail',\n",
       " '.',\n",
       " 'Thanks',\n",
       " ',',\n",
       " '-',\n",
       " 'IL',\n",
       " '--',\n",
       " '--',\n",
       " 'brought',\n",
       " 'to',\n",
       " 'you',\n",
       " 'by',\n",
       " 'your',\n",
       " 'neighborhood',\n",
       " 'Lerxst',\n",
       " '--',\n",
       " '--']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "tok = word_tokenize(article)\n",
    "tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pksduBGKFMxt"
   },
   "source": [
    "\n",
    "- **Lemmatization (nltk):**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rochi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['From',\n",
       " ':',\n",
       " 'lerxst',\n",
       " '@',\n",
       " 'wam.umd.edu',\n",
       " '(',\n",
       " 'where',\n",
       " \"'s\",\n",
       " 'my',\n",
       " 'thing',\n",
       " ')',\n",
       " 'Subject',\n",
       " ':',\n",
       " 'WHAT',\n",
       " 'car',\n",
       " 'be',\n",
       " 'this',\n",
       " '!',\n",
       " '?',\n",
       " 'Nntp-Posting-Host',\n",
       " ':',\n",
       " 'rac3.wam.umd.edu',\n",
       " 'Organization',\n",
       " ':',\n",
       " 'University',\n",
       " 'of',\n",
       " 'Maryland',\n",
       " ',',\n",
       " 'College',\n",
       " 'Park',\n",
       " 'Lines',\n",
       " ':',\n",
       " '15',\n",
       " 'I',\n",
       " 'be',\n",
       " 'wonder',\n",
       " 'if',\n",
       " 'anyone',\n",
       " 'out',\n",
       " 'there',\n",
       " 'could',\n",
       " 'enlighten',\n",
       " 'me',\n",
       " 'on',\n",
       " 'this',\n",
       " 'car',\n",
       " 'I',\n",
       " 'saw',\n",
       " 'the',\n",
       " 'other',\n",
       " 'day',\n",
       " '.',\n",
       " 'It',\n",
       " 'be',\n",
       " 'a',\n",
       " '2-door',\n",
       " 'sport',\n",
       " 'car',\n",
       " ',',\n",
       " 'look',\n",
       " 'to',\n",
       " 'be',\n",
       " 'from',\n",
       " 'the',\n",
       " 'late',\n",
       " '60s/',\n",
       " 'early',\n",
       " '70s',\n",
       " '.',\n",
       " 'It',\n",
       " 'be',\n",
       " 'call',\n",
       " 'a',\n",
       " 'Bricklin',\n",
       " '.',\n",
       " 'The',\n",
       " 'doors',\n",
       " 'be',\n",
       " 'really',\n",
       " 'small',\n",
       " '.',\n",
       " 'In',\n",
       " 'addition',\n",
       " ',',\n",
       " 'the',\n",
       " 'front',\n",
       " 'bumper',\n",
       " 'be',\n",
       " 'separate',\n",
       " 'from',\n",
       " 'the',\n",
       " 'rest',\n",
       " 'of',\n",
       " 'the',\n",
       " 'body',\n",
       " '.',\n",
       " 'This',\n",
       " 'be',\n",
       " 'all',\n",
       " 'I',\n",
       " 'know',\n",
       " '.',\n",
       " 'If',\n",
       " 'anyone',\n",
       " 'can',\n",
       " 'tellme',\n",
       " 'a',\n",
       " 'model',\n",
       " 'name',\n",
       " ',',\n",
       " 'engine',\n",
       " 'specs',\n",
       " ',',\n",
       " 'years',\n",
       " 'of',\n",
       " 'production',\n",
       " ',',\n",
       " 'where',\n",
       " 'this',\n",
       " 'car',\n",
       " 'be',\n",
       " 'make',\n",
       " ',',\n",
       " 'history',\n",
       " ',',\n",
       " 'or',\n",
       " 'whatever',\n",
       " 'info',\n",
       " 'you',\n",
       " 'have',\n",
       " 'on',\n",
       " 'this',\n",
       " 'funky',\n",
       " 'look',\n",
       " 'car',\n",
       " ',',\n",
       " 'please',\n",
       " 'e-mail',\n",
       " '.',\n",
       " 'Thanks',\n",
       " ',',\n",
       " '-',\n",
       " 'IL',\n",
       " '--',\n",
       " '--',\n",
       " 'bring',\n",
       " 'to',\n",
       " 'you',\n",
       " 'by',\n",
       " 'your',\n",
       " 'neighborhood',\n",
       " 'Lerxst',\n",
       " '--',\n",
       " '--']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lem=[lemmatizer.lemmatize(x,pos='v') for x in tok]\n",
    "lem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-zxqqFYAFOsN"
   },
   "source": [
    "- **Stop Words (nltk):**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rochi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['From',\n",
       " ':',\n",
       " 'lerxst',\n",
       " '@',\n",
       " 'wam.umd.edu',\n",
       " '(',\n",
       " \"'s\",\n",
       " 'thing',\n",
       " ')',\n",
       " 'Subject',\n",
       " ':',\n",
       " 'WHAT',\n",
       " 'car',\n",
       " '!',\n",
       " '?',\n",
       " 'Nntp-Posting-Host',\n",
       " ':',\n",
       " 'rac3.wam.umd.edu',\n",
       " 'Organization',\n",
       " ':',\n",
       " 'University',\n",
       " 'Maryland',\n",
       " ',',\n",
       " 'College',\n",
       " 'Park',\n",
       " 'Lines',\n",
       " ':',\n",
       " '15',\n",
       " 'I',\n",
       " 'wonder',\n",
       " 'anyone',\n",
       " 'could',\n",
       " 'enlighten',\n",
       " 'car',\n",
       " 'I',\n",
       " 'saw',\n",
       " 'day',\n",
       " '.',\n",
       " 'It',\n",
       " '2-door',\n",
       " 'sport',\n",
       " 'car',\n",
       " ',',\n",
       " 'look',\n",
       " 'late',\n",
       " '60s/',\n",
       " 'early',\n",
       " '70s',\n",
       " '.',\n",
       " 'It',\n",
       " 'call',\n",
       " 'Bricklin',\n",
       " '.',\n",
       " 'The',\n",
       " 'doors',\n",
       " 'really',\n",
       " 'small',\n",
       " '.',\n",
       " 'In',\n",
       " 'addition',\n",
       " ',',\n",
       " 'front',\n",
       " 'bumper',\n",
       " 'separate',\n",
       " 'rest',\n",
       " 'body',\n",
       " '.',\n",
       " 'This',\n",
       " 'I',\n",
       " 'know',\n",
       " '.',\n",
       " 'If',\n",
       " 'anyone',\n",
       " 'tellme',\n",
       " 'model',\n",
       " 'name',\n",
       " ',',\n",
       " 'engine',\n",
       " 'specs',\n",
       " ',',\n",
       " 'years',\n",
       " 'production',\n",
       " ',',\n",
       " 'car',\n",
       " 'make',\n",
       " ',',\n",
       " 'history',\n",
       " ',',\n",
       " 'whatever',\n",
       " 'info',\n",
       " 'funky',\n",
       " 'look',\n",
       " 'car',\n",
       " ',',\n",
       " 'please',\n",
       " 'e-mail',\n",
       " '.',\n",
       " 'Thanks',\n",
       " ',',\n",
       " '-',\n",
       " 'IL',\n",
       " '--',\n",
       " '--',\n",
       " 'bring',\n",
       " 'neighborhood',\n",
       " 'Lerxst',\n",
       " '--',\n",
       " '--']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop = [x for x in lem if x not in stopwords.words('english')]\n",
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UjKQKWLGFQRl"
   },
   "source": [
    "- **Stemming (nltk):**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['from',\n",
       " ':',\n",
       " 'lerxst',\n",
       " '@',\n",
       " 'wam.umd.edu',\n",
       " '(',\n",
       " \"'s\",\n",
       " 'thing',\n",
       " ')',\n",
       " 'subject',\n",
       " ':',\n",
       " 'what',\n",
       " 'car',\n",
       " '!',\n",
       " '?',\n",
       " 'nntp-posting-host',\n",
       " ':',\n",
       " 'rac3.wam.umd.edu',\n",
       " 'organ',\n",
       " ':',\n",
       " 'univers',\n",
       " 'maryland',\n",
       " ',',\n",
       " 'colleg',\n",
       " 'park',\n",
       " 'line',\n",
       " ':',\n",
       " '15',\n",
       " 'I',\n",
       " 'wonder',\n",
       " 'anyon',\n",
       " 'could',\n",
       " 'enlighten',\n",
       " 'car',\n",
       " 'I',\n",
       " 'saw',\n",
       " 'day',\n",
       " '.',\n",
       " 'It',\n",
       " '2-door',\n",
       " 'sport',\n",
       " 'car',\n",
       " ',',\n",
       " 'look',\n",
       " 'late',\n",
       " '60s/',\n",
       " 'earli',\n",
       " '70',\n",
       " '.',\n",
       " 'It',\n",
       " 'call',\n",
       " 'bricklin',\n",
       " '.',\n",
       " 'the',\n",
       " 'door',\n",
       " 'realli',\n",
       " 'small',\n",
       " '.',\n",
       " 'In',\n",
       " 'addit',\n",
       " ',',\n",
       " 'front',\n",
       " 'bumper',\n",
       " 'separ',\n",
       " 'rest',\n",
       " 'bodi',\n",
       " '.',\n",
       " 'thi',\n",
       " 'I',\n",
       " 'know',\n",
       " '.',\n",
       " 'If',\n",
       " 'anyon',\n",
       " 'tellm',\n",
       " 'model',\n",
       " 'name',\n",
       " ',',\n",
       " 'engin',\n",
       " 'spec',\n",
       " ',',\n",
       " 'year',\n",
       " 'product',\n",
       " ',',\n",
       " 'car',\n",
       " 'make',\n",
       " ',',\n",
       " 'histori',\n",
       " ',',\n",
       " 'whatev',\n",
       " 'info',\n",
       " 'funki',\n",
       " 'look',\n",
       " 'car',\n",
       " ',',\n",
       " 'pleas',\n",
       " 'e-mail',\n",
       " '.',\n",
       " 'thank',\n",
       " ',',\n",
       " '-',\n",
       " 'IL',\n",
       " '--',\n",
       " '--',\n",
       " 'bring',\n",
       " 'neighborhood',\n",
       " 'lerxst',\n",
       " '--',\n",
       " '--']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stem=[stemmer.stem(x) for x in stop]\n",
    "stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HNfMRbqmFSIR"
   },
   "source": [
    "- **Filtrado de palabras:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['from',\n",
       " 'lerxst',\n",
       " 'thing',\n",
       " 'subject',\n",
       " 'what',\n",
       " 'car',\n",
       " 'organ',\n",
       " 'univers',\n",
       " 'maryland',\n",
       " 'colleg',\n",
       " 'park',\n",
       " 'line',\n",
       " 'I',\n",
       " 'wonder',\n",
       " 'anyon',\n",
       " 'could',\n",
       " 'enlighten',\n",
       " 'car',\n",
       " 'I',\n",
       " 'saw',\n",
       " 'day',\n",
       " 'It',\n",
       " 'sport',\n",
       " 'car',\n",
       " 'look',\n",
       " 'late',\n",
       " 'earli',\n",
       " 'It',\n",
       " 'call',\n",
       " 'bricklin',\n",
       " 'the',\n",
       " 'door',\n",
       " 'realli',\n",
       " 'small',\n",
       " 'In',\n",
       " 'addit',\n",
       " 'front',\n",
       " 'bumper',\n",
       " 'separ',\n",
       " 'rest',\n",
       " 'bodi',\n",
       " 'thi',\n",
       " 'I',\n",
       " 'know',\n",
       " 'If',\n",
       " 'anyon',\n",
       " 'tellm',\n",
       " 'model',\n",
       " 'name',\n",
       " 'engin',\n",
       " 'spec',\n",
       " 'year',\n",
       " 'product',\n",
       " 'car',\n",
       " 'make',\n",
       " 'histori',\n",
       " 'whatev',\n",
       " 'info',\n",
       " 'funki',\n",
       " 'look',\n",
       " 'car',\n",
       " 'pleas',\n",
       " 'thank',\n",
       " 'IL',\n",
       " 'bring',\n",
       " 'neighborhood',\n",
       " 'lerxst']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = [x for x in stem if x.isalpha()]\n",
    "alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yOzGVenvIESr"
   },
   "source": [
    "### Preprocesamiento completo\n",
    "\n",
    "Utilizar o no cada uno de los métodos vistos es una decisión que dependerá del caso particular de aplicación. Para este ejercicio vamos a considerar las siguientes combinaciones:\n",
    "\n",
    "- Tokenización\n",
    "- Tokenización, Lematización, Stemming.\n",
    "- Tokenización, Stop Words.\n",
    "- Tokenización, Lematización, Stop Words, Stemming.\n",
    "- Tokenización, Lematización, Stop Words, Stemming, Filtrado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PyC4gxFzRE0B"
   },
   "outputs": [],
   "source": [
    "#Solución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wP3n8ZwFc7_D"
   },
   "source": [
    "Preguntas\n",
    "\n",
    "- Cómo cambia el tamaño del vocabulario al agregar Lematización y Stemming?\n",
    "- Cómo cambia el tamaño del vocabulario al Stop Words?\n",
    "- Analice muy brevemente ventajas y desventajas del tamaño del dataset en cada caso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TgrHtzEuIqid"
   },
   "source": [
    "### Guardado de pre procesamiento\n",
    "Vamos a guardar lo preprocesado usando pickle, que nos permite serializar objetos y guardarlos en disco, es muy importante que sepan hacer esto si no quieren perder tiempo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ofiOlGS4SsgL"
   },
   "outputs": [],
   "source": [
    "#Salvado del procesamiento a disco:\n",
    "import pickle\n",
    "\n",
    "with open('art_filt.pck', 'wb') as fp:\n",
    "    pickle.dump(emails_filtrados, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_duXZlcPSxTU"
   },
   "outputs": [],
   "source": [
    "with open ('art_filt.pck', 'rb') as fp:\n",
    "    itemlist = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c-R-LPRoIV0E"
   },
   "source": [
    "## Vectorización de texto\n",
    "\n",
    "- **Obtención del vocabulario y obtención de la probabilidad**\n",
    "\n",
    "Como se vió en clase, los vectorizadores cuentan con dos parámetros de ajuste.\n",
    "\n",
    "- max_df: le asignamos una maxima frecuencia de aparición, eliminando las palabras comunes que no aportan información.\n",
    "\n",
    "- min_df: le asignamos la minima cantidad de veces que tiene que aparecer una palabra.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4w4SCTsYaHF8"
   },
   "source": [
    "Al igual que con las diferentes opciones de preprocesamiento, lo mismo ocurre con la vectorización. Podemos utilizar CountVectorizer o TfidfVectorizer según el caso (con diferentes valores de max_df y min_df).\n",
    "Para este ejercicio deben utilizar ambos métodos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C74USIEyS3B7"
   },
   "outputs": [],
   "source": [
    "#Solución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RsDrS0EvTOY-"
   },
   "outputs": [],
   "source": [
    "raw_data.shape #Para cada documento hay un vector de ocurrencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mZjO8CFPTUM6"
   },
   "outputs": [],
   "source": [
    "raw_data.toarray() #Es una sparse matrix, vamos a expandirla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3iqIoNyTTYVc"
   },
   "outputs": [],
   "source": [
    "raw_data.toarray()[0,:].argmax() #Veamos a qué palabra pertenece la máxima ocurrencia en el primer artíclo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G7ob-IgsMRS7"
   },
   "source": [
    "## Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a4bVz9QwMupE"
   },
   "source": [
    "Primero deben separar correctamente el dataset para hacer validación del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5S9rAz2tNvkw"
   },
   "source": [
    "Y luego deben entrenar el modelo de NaiveBayes con el dataset de train.\n",
    "\n",
    "Deben utilizar un modelo de NaiveBayes Multinomial y de Bernoulli. Ambos modelos estan disponibles en sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mDUWaF5_VB4K"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "\n",
    "#Completar para cada caso según corresponda\n",
    "clf = MultinomialNB()\n",
    "# clf = BernoulliNB()\n",
    "clf.fit(X_train_data.toarray(), Y_train_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "692nzR2JOV1F"
   },
   "source": [
    "Finalmente comprobar el accuracy en train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vbidxDQ7VPCb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "porc=sum(np.array(clf.predict(X_val_train.toarray()))==np.array(Y_train_data))/len(Y_train_data)*100\n",
    "print(\"El porcentaje de artículos clasificados correctamente es: {:.2f}%\".format(porc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GPPo4Fr-eSON"
   },
   "source": [
    "Preguntas\n",
    "\n",
    "- Con que combinación de preprocesamiento obtuvo los mejores resultados? Explique por qué cree que fue así.\n",
    "\n",
    "- Con que modelo obtuvo los mejores resultados? Explique por qué cree que fue así."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mOGoiDhGe4MH"
   },
   "source": [
    "## Performance de los modelos\n",
    "\n",
    "En el caso anterior, para medir la cantidad de artículos clasiicados correctamente se utilizó el mismo subconjunto del dataset que se utilizó para entrenar.\n",
    "\n",
    "Esta medida no es una medida del todo útil, ya que lo que interesa de un clasificador es su capacidad de clasificación de datos que no fueron utilizados para entrenar. Es por eso que se pide, para el clasificador entrenado con el subconjunto de training, cual es el porcentaje de artículos del subconjunto de testing clasificados correctamente. Comparar con el porcentaje anterior y explicar las diferencias.\n",
    "\n",
    "Finalmente deben observar las diferencias y extraer conclusiones en base al accuracy obtenido, el preprocesamiento y vectorización utilizado y el modelo, para cada combinación de posibilidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fsnhfAiwg-A-"
   },
   "outputs": [],
   "source": [
    "# Solución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UpphW8cAVcrX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "porc=sum(np.array(clf.predict(X_test.toarray()))==np.array(Y_test))/len(Y_test)*100\n",
    "print(\"El porcentaje de artículos clasificados correctamente es: {:.2f}%\".format(porc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iOf-BcvpeqKp"
   },
   "source": [
    "Preguntas\n",
    "\n",
    "- El accuracy en el dataset de test es mayor o menor que en train? Explique por qué."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ejercicio_Clasificación_Texto.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
