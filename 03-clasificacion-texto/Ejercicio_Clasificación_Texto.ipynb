{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8mBlkilaNC2M"
   },
   "outputs": [],
   "source": [
    "# Estos dos comandos evitan que haya que hacer reload cada vez que se modifica un paquete\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4XI-bramQQLk"
   },
   "source": [
    "# Ejercicio de clasificación de texto\n",
    "\n",
    "Naive Bayes es una técnica estadística que consiste en repetir el método anterior en problemas cuyos sucesos no son independientes, pero suponiendo independencia.\n",
    "A lo largo de este trabajo desarrollarán un modelo de Naive Bayes para el problema de clasificación de artículos periodístios.En este caso podemos estimar la probabilidad de ocurrencia de cada palabra según la categoría a la que pertenece el artículo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zJ7IlPHXEDEJ"
   },
   "source": [
    "## Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z8adA_beEXG7"
   },
   "source": [
    "El primer paso es obtener el dataset que vamos a utilizar. El dataset a utilizar es el de TwentyNewsGroup(TNG) que está disponible en sklearn.\n",
    "\n",
    "Se puede encontrar más información del dataset en la documentación de scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from helper import autosave\n",
    "\n",
    "\n",
    "@autosave('twenty-md={metadata}-{subset}')\n",
    "def get_20newsgroup(subset='train', metadata=False):\n",
    "    if metadata:\n",
    "        return fetch_20newsgroups(subset=subset, shuffle=True)\n",
    "    else:\n",
    "        return fetch_20newsgroups(subset=subset, remove=('headers', 'footers', 'quotes'), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "AHaexjmQNQej",
    "outputId": "2dd3fd54-8454-4dde-8c08-d2b917679fdf"
   },
   "outputs": [],
   "source": [
    "#Loading the data set - training data.\n",
    "\n",
    "twenty_train = get_20newsgroup(subset='train', metadata=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u65YieUTEgR3"
   },
   "source": [
    "El siguiente paso es analizar el contenido del dataset, como por ejemplo la cantidad de artículos, la cantidad de clases, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sx2_w9hbS9-D"
   },
   "source": [
    "Preguntas:\n",
    "\n",
    "1) ¿Cuántos articulos tiene el dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dmBRcNNbRvVu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twenty_train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) ¿Cuántas clases tiene el dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twenty_train.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) ¿Es un dataset balanceado?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El dataset no está balanceado\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "_, counts = np.unique(twenty_train.target, return_counts=True)\n",
    "if len(set(counts)) == 1:\n",
    "    print('El dataset está balanceado')\n",
    "else:    \n",
    "    print('El dataset no está balanceado')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) ¿Cuál es la probabilidad a priori de la clase 5? A que corresponde esta clase?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La clase 5 (comp.windows.x) tiene probabilidad a priori 0.05241293972070002\n"
     ]
    }
   ],
   "source": [
    "priori5 = counts[5]/sum(counts)\n",
    "print(f'La clase 5 ({twenty_train.target_names[5]}) tiene probabilidad a priori {priori5:3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) ¿Cuál es la clase con mayor probabilidad a priori?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La clase 10 (rec.sport.hockey) tiene la máxima probabilidad a priori, 0.053\n"
     ]
    }
   ],
   "source": [
    "max_class = np.argmax(counts)\n",
    "print(f'La clase {max_class} ({twenty_train.target_names[max_class]}) tiene la máxima probabilidad a priori, {counts[max_class]/sum(counts):.2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84hQvk-UHX1V"
   },
   "source": [
    "## Preprocesamiento\n",
    "\n",
    "Para facilitar la comprensión de los algoritmos de preprocesamiento, se aplican primero a un solo artículo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l0zxZSOsFXpl"
   },
   "source": [
    "\n",
    "Mas info en:\n",
    "http://text-processing.com/demo/stem/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rochi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rochi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rochi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from helper import print_list\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "article = twenty_train.data[0]\n",
    "article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-geItZPyUWEO"
   },
   "source": [
    "- **Tokenization (nltk):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'I', 'saw', 'the', 'other', 'day', '.', 'It', 'was', 'a', '2-door', 'sports', 'car', ',', 'looked', 'to', 'be', 'from', 'the', 'late', '60s/', 'early', '70s', '.', 'It', 'was', 'called', 'a', 'Bricklin', '.', 'The', 'doors', 'were', 'really', 'small', '.', 'In', 'addition', ',', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', '.', 'This', 'is', 'all', 'I', 'know', '.', 'If', 'anyone', 'can', 'tellme', 'a', 'model', 'name', ',', 'engine', 'specs', ',', 'years', 'of', 'production', ',', 'where', 'this', 'car', 'is', 'made', ',', 'history', ',', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', ',', 'please', 'e-mail', '.']\n",
      "Palabras totales: 106\n",
      "Palabras distintas: 71\n"
     ]
    }
   ],
   "source": [
    "tok = word_tokenize(article)\n",
    "print_list(tok)\n",
    "# print feo porque me ponia enters y quedaba muy largo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pksduBGKFMxt"
   },
   "source": [
    "- **Lemmatization (nltk):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'be', 'wonder', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'I', 'saw', 'the', 'other', 'day', '.', 'It', 'be', 'a', '2-door', 'sport', 'car', ',', 'look', 'to', 'be', 'from', 'the', 'late', '60s/', 'early', '70s', '.', 'It', 'be', 'call', 'a', 'Bricklin', '.', 'The', 'doors', 'be', 'really', 'small', '.', 'In', 'addition', ',', 'the', 'front', 'bumper', 'be', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', '.', 'This', 'be', 'all', 'I', 'know', '.', 'If', 'anyone', 'can', 'tellme', 'a', 'model', 'name', ',', 'engine', 'specs', ',', 'years', 'of', 'production', ',', 'where', 'this', 'car', 'be', 'make', ',', 'history', ',', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'look', 'car', ',', 'please', 'e-mail', '.']\n",
      "Palabras totales: 106\n",
      "Palabras distintas: 67\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lem=[lemmatizer.lemmatize(x, pos='v') for x in tok]\n",
    "print_list(lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-zxqqFYAFOsN"
   },
   "source": [
    "- **Stop Words (nltk):**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'wonder', 'anyone', 'could', 'enlighten', 'car', 'I', 'saw', 'day', '.', 'It', '2-door', 'sport', 'car', ',', 'look', 'late', '60s/', 'early', '70s', '.', 'It', 'call', 'Bricklin', '.', 'The', 'doors', 'really', 'small', '.', 'In', 'addition', ',', 'front', 'bumper', 'separate', 'rest', 'body', '.', 'This', 'I', 'know', '.', 'If', 'anyone', 'tellme', 'model', 'name', ',', 'engine', 'specs', ',', 'years', 'production', ',', 'car', 'make', ',', 'history', ',', 'whatever', 'info', 'funky', 'look', 'car', ',', 'please', 'e-mail', '.']\n",
      "Palabras totales: 69\n",
      "Palabras distintas: 48\n"
     ]
    }
   ],
   "source": [
    "stop = [x for x in lem if x not in stopwords.words('english')]\n",
    "print_list(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    }
   ],
   "source": [
    "print(len(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UjKQKWLGFQRl"
   },
   "source": [
    "- **Stemming (nltk):**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'wonder', 'anyon', 'could', 'enlighten', 'car', 'I', 'saw', 'day', '.', 'It', '2-door', 'sport', 'car', ',', 'look', 'late', '60s/', 'earli', '70', '.', 'It', 'call', 'bricklin', '.', 'the', 'door', 'realli', 'small', '.', 'In', 'addit', ',', 'front', 'bumper', 'separ', 'rest', 'bodi', '.', 'thi', 'I', 'know', '.', 'If', 'anyon', 'tellm', 'model', 'name', ',', 'engin', 'spec', ',', 'year', 'product', ',', 'car', 'make', ',', 'histori', ',', 'whatev', 'info', 'funki', 'look', 'car', ',', 'pleas', 'e-mail', '.']\n",
      "Palabras totales: 69\n",
      "Palabras distintas: 48\n"
     ]
    }
   ],
   "source": [
    "stem = [stemmer.stem(x) for x in stop]\n",
    "print_list(stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HNfMRbqmFSIR"
   },
   "source": [
    "- **Filtrado de palabras:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'wonder', 'anyon', 'could', 'enlighten', 'car', 'I', 'saw', 'day', 'It', 'sport', 'car', 'look', 'late', 'earli', 'It', 'call', 'bricklin', 'the', 'door', 'realli', 'small', 'In', 'addit', 'front', 'bumper', 'separ', 'rest', 'bodi', 'thi', 'I', 'know', 'If', 'anyon', 'tellm', 'model', 'name', 'engin', 'spec', 'year', 'product', 'car', 'make', 'histori', 'whatev', 'info', 'funki', 'look', 'car', 'pleas']\n",
      "Palabras totales: 50\n",
      "Palabras distintas: 42\n"
     ]
    }
   ],
   "source": [
    "alpha = [x for x in stem if x.isalpha()]\n",
    "print_list(alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yOzGVenvIESr"
   },
   "source": [
    "### Preprocesamiento completo\n",
    "\n",
    "Utilizar o no cada uno de los métodos vistos es una decisión que dependerá del caso particular de aplicación. Para este ejercicio vamos a considerar las siguientes combinaciones:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenización\n",
    "- Tokenización, Lematización, Stemming.\n",
    "- Tokenización, Stop Words.\n",
    "- Tokenización, Lematización, Stop Words, Stemming.\n",
    "- Tokenización, Lematización, Stop Words, Stemming, Filtrado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_article(article, filts):\n",
    "    filts = filts.split()\n",
    "\n",
    "    if 'lem' in filts:\n",
    "        article = [lemmatizer.lemmatize(x,pos='v') for x in article]\n",
    "\n",
    "    if 'stop' in filts:\n",
    "        article = [x for x in article if x not in stopwords.words('english')]\n",
    "\n",
    "    if 'stem' in filts:\n",
    "        article = [stemmer.stem(x) for x in article]\n",
    "\n",
    "    if 'filt' in filts:\n",
    "        article = [x for x in article if x.isalpha()]\n",
    "    \n",
    "    return article\n",
    "\n",
    "\n",
    "@autosave(fmt='{name}-{filts}')\n",
    "def filter_articles(name, articles, filts):\n",
    "    \n",
    "    filtered_articles = []\n",
    "    for data in articles:\n",
    "        tok = word_tokenize(data)\n",
    "        filtered_articles.append(filter_article(tok, filts))\n",
    "            \n",
    "    return filtered_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocesamiento: tok\n",
      "Longitud del vocabulario: 161698\n",
      "\n",
      "Preprocesamiento: tok lem stem\n",
      "Longitud del vocabulario: 126903\n",
      "\n",
      "Preprocesamiento: tok stop\n",
      "Longitud del vocabulario: 161533\n",
      "\n",
      "Preprocesamiento: tok lem stop stem\n",
      "Longitud del vocabulario: 126872\n",
      "\n",
      "Preprocesamiento: tok lem stop stem filt\n",
      "Longitud del vocabulario: 45315\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ans_fmt = \"\"\"Preprocesamiento: {preproc}\n",
    "Longitud del vocabulario: {vocab_len}\n",
    "\"\"\"\n",
    "preprocessing = ['tok', 'tok lem stem', 'tok stop', 'tok lem stop stem', 'tok lem stop stem filt']\n",
    "\n",
    "    \n",
    "for preproc in preprocessing:\n",
    "    filtered_articles = filter_articles('train-nometadata', twenty_train.data, preproc)\n",
    "    vocab = set([word for article in filtered_articles for word in article])\n",
    "    print(ans_fmt.format(preproc=preproc, vocab_len=len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wP3n8ZwFc7_D"
   },
   "source": [
    "Preguntas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cómo cambia el tamaño del vocabulario al agregar Lematización y Stemming?\n",
    "\n",
    "Sólo con tokenización: 161698 palabras\n",
    "\n",
    "Con lematización y stemming: 126903 palabras (21.5% menor, 34795 palabras menos).\n",
    "\n",
    "El vocabulario se reduce no porque la cantidad total de palabras sea menor, sino porque palabras que originalmente eran distintas ahora son iguales (por ejemplo, \"is\" y \"was\" se convierten ambas en \"be\").\n",
    "\n",
    "La reducción del 20% sugiere que de cada 5 palabras, dos son dos \"versiones\" de la misma, lo cual es razonable si se tiene en cuenta que es muy común usar una misma palabra en singular y plural en un mismo contexto (\"car\" / \"cars\"), adverbios y adjetivos con la misma raíz (\"real\" / \"really\"), pronombres en distintos casos (\"I\" / \"me\", etcétera (en español esto probablemente sería incluso más pronunciado, al haber mayor cantidad de declinaciones verbales distintas, y tener géneros para adjetivos y artículos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cómo cambia el tamaño del vocabulario al Stop Words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sólo con tokenización: 161698 palabras\n",
    "\n",
    "Con stop words: 161533 palabras (0.1% menor, 165 palabras menos)\n",
    "\n",
    "El vocabulario se reduce porque se remueven las palabras contenidas en el conjunto de stop words. Es razonable entonces que la reducción del vocabulario sea menor de este caso, ya que como máximo se podrán remover tantas palabras como haya en la lista de stop words (en este caso, 179)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Analice muy brevemente ventajas y desventajas del tamaño del dataset en cada caso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el caso de stopwords, es útil porque es razonable pensar que ese conjunto de palabras estará presente en todos los artículos, y por lo tanto no aportará demasiada información. Por ejemplo, una palabra como \"car\", \"player\" o \"God\" nos da más información sobre de qué se está hablando que \"in\", \"no\" o \"is\".\n",
    "\n",
    "Sin embargo, también pueden imaginarse casos donde esto no sea cierto. Por ejemplo, si una clase se caracteriza por narraciones en primera persona, mientras que los demás suelen ser más impersonales, se estaría ignorando la información que aportan palabras como \"I\", \"me\", etc. Lo mismo puede ser cierto de segunda persona, o tercera persona \"he/she/they\" (en artículos que hablan de personas) en lugar de cosas (\"it\").\n",
    "\n",
    "En cuanto a la lematización y stemming, puede hacerse un análisis muy similar. En muchos casos, es muy útil combinar las probabilidades de palabras similares: en un artículo de deportes, la palabra \"partido\" o \"partidos\" apuntan a un contexto muy similar, así como hablar de un evento que sucederá en el futuro, anticipando y especulando (\"if they win\"), o haciendo un racconto de eventos del pasado (\"after they won\") no es particularmente relevante.\n",
    "\n",
    "Sin embargo, nuevamente pueden pensarse casos donde esto no será cierto: puede que una clase se caracterice por hablar sobre eventos del pasado, mientras que otra se dedique más a explicaciones en presente, y ese matiz se pierde."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c-R-LPRoIV0E"
   },
   "source": [
    "## Vectorización de texto\n",
    "\n",
    "- **Obtención del vocabulario y obtención de la probabilidad**\n",
    "\n",
    "Como se vió en clase, los vectorizadores cuentan con dos parámetros de ajuste.\n",
    "\n",
    "- max_df: le asignamos una maxima frecuencia de aparición, eliminando las palabras comunes que no aportan información.\n",
    "\n",
    "- min_df: le asignamos la minima cantidad de veces que tiene que aparecer una palabra.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@autosave(fmt='{name}-joined-{filts}')\n",
    "def get_filtered_joined_articles(name, articles, filts):\n",
    "    articles = filter_articles(name, articles, filts)\n",
    "    for i in range(len(articles)):\n",
    "        articles[i] = ' '.join(articles[i])\n",
    "        \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C74USIEyS3B7"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "import pandas as pd\n",
    "import os.path\n",
    "\n",
    "\n",
    "OUT_FILE = 'out-nometadata5.csv'\n",
    "\n",
    "vectorizers = [CountVectorizer, TfidfVectorizer]\n",
    "classifiers = [MultinomialNB, BernoulliNB]\n",
    "max_dfs = [0.005,0.01, 0.025, 0.04, .05, 0.1]\n",
    "min_dfs = [1, 2, 5, 10]\n",
    "\n",
    "results = []\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "total_its = len(vectorizers)*len(classifiers)*len(min_dfs)*len(max_dfs)*len(preprocessing)\n",
    "i = 1\n",
    "\n",
    "if os.path.isfile(OUT_FILE):\n",
    "    results = pd.read_csv(OUT_FILE)\n",
    "else:\n",
    "    results = []\n",
    "\n",
    "    for filts in preprocessing:\n",
    "        data = get_filtered_joined_articles(name='train-nometadata', filts=filts) \n",
    "        \n",
    "        for max_df in max_dfs:\n",
    "            for min_df in min_dfs:\n",
    "                \n",
    "                for Vectorizer in vectorizers:\n",
    "                    count_vect = Vectorizer(max_df=max_df, min_df=min_df)\n",
    "                    raw_data = count_vect.fit_transform(data) \n",
    "\n",
    "                    for Classifier in classifiers:\n",
    "                        print(f'Computing: F={filts}, M={max_df}, m={min_df}, V={Vectorizer.__name__}, C={Classifier.__name__} ({i}/{total_its})...')\n",
    "                        i += 1\n",
    "                        \n",
    "                        clf = Classifier()\n",
    "                        clf.fit(raw_data, twenty_train.target)\n",
    "                        score = clf.score(raw_data, twenty_train.target)\n",
    "\n",
    "                        results.append({\n",
    "                            'max_df': max_df,\n",
    "                            'min_df': min_df,\n",
    "                            'filts': filts,\n",
    "                            'vectorizer': Vectorizer.__name__,\n",
    "                            'model': Classifier.__name__,\n",
    "                            'score': score\n",
    "                        })\n",
    "                        \n",
    "                        print(f'Score: {score:.5%}')\n",
    "                        print(50 * '-')\n",
    "    results = pd.DataFrame(results)\n",
    "    results.to_csv(OUT_FILE, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values(by=['score'], ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_df</th>\n",
       "      <th>min_df</th>\n",
       "      <th>filts</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>model</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>0.95</td>\n",
       "      <td>1</td>\n",
       "      <td>tok lem stem</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.723528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>0.80</td>\n",
       "      <td>1</td>\n",
       "      <td>tok lem stem</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.734135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>0.75</td>\n",
       "      <td>1</td>\n",
       "      <td>tok lem stem</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.734135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.95</td>\n",
       "      <td>1</td>\n",
       "      <td>tok</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.745183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.75</td>\n",
       "      <td>1</td>\n",
       "      <td>tok</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.751635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     max_df  min_df         filts       vectorizer          model     score\n",
       "240    0.95       1  tok lem stem  CountVectorizer  MultinomialNB  0.723528\n",
       "224    0.80       1  tok lem stem  CountVectorizer  MultinomialNB  0.734135\n",
       "208    0.75       1  tok lem stem  CountVectorizer  MultinomialNB  0.734135\n",
       "112    0.95       1           tok  CountVectorizer  MultinomialNB  0.745183\n",
       "80     0.75       1           tok  CountVectorizer  MultinomialNB  0.751635"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[results.model=='MultinomialNB'].sort_values(by='score', ascending=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_df</th>\n",
       "      <th>min_df</th>\n",
       "      <th>filts</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>model</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>0.05</td>\n",
       "      <td>5</td>\n",
       "      <td>tok lem stop stem filt</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.682871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>0.05</td>\n",
       "      <td>5</td>\n",
       "      <td>tok lem stop stem filt</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.682871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>0.05</td>\n",
       "      <td>10</td>\n",
       "      <td>tok lem stop stem filt</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.678451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>0.05</td>\n",
       "      <td>10</td>\n",
       "      <td>tok lem stop stem filt</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.678451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.05</td>\n",
       "      <td>10</td>\n",
       "      <td>tok</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.678363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     max_df  min_df                   filts       vectorizer        model  \\\n",
       "523    0.05       5  tok lem stop stem filt  TfidfVectorizer  BernoulliNB   \n",
       "521    0.05       5  tok lem stop stem filt  CountVectorizer  BernoulliNB   \n",
       "525    0.05      10  tok lem stop stem filt  CountVectorizer  BernoulliNB   \n",
       "527    0.05      10  tok lem stop stem filt  TfidfVectorizer  BernoulliNB   \n",
       "13     0.05      10                     tok  CountVectorizer  BernoulliNB   \n",
       "\n",
       "        score  \n",
       "523  0.682871  \n",
       "521  0.682871  \n",
       "525  0.678451  \n",
       "527  0.678451  \n",
       "13   0.678363  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[results.model=='BernoulliNB'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "640"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3iqIoNyTTYVc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_df</th>\n",
       "      <th>min_df</th>\n",
       "      <th>filts</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>model</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.05</td>\n",
       "      <td>1</td>\n",
       "      <td>tok</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.884568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   max_df  min_df filts       vectorizer          model     score\n",
       "2    0.05       1   tok  TfidfVectorizer  MultinomialNB  0.884568"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best = results['score'].max()\n",
    "best_filt = results['score']==best\n",
    "best_hypers = results[best_filt]\n",
    "best_hypers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyper = best_hypers.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G7ob-IgsMRS7"
   },
   "source": [
    "## Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a4bVz9QwMupE"
   },
   "source": [
    "Primero deben separar correctamente el dataset para hacer validación del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5S9rAz2tNvkw"
   },
   "source": [
    "Y luego deben entrenar el modelo de NaiveBayes con el dataset de train.\n",
    "\n",
    "Deben utilizar un modelo de NaiveBayes Multinomial y de Bernoulli. Ambos modelos estan disponibles en sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "692nzR2JOV1F"
   },
   "source": [
    "Finalmente comprobar el accuracy en train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GPPo4Fr-eSON"
   },
   "source": [
    "Preguntas\n",
    "\n",
    "- Con que combinación de preprocesamiento obtuvo los mejores resultados? Explique por qué cree que fue así.\n",
    "\n",
    "- Con que modelo obtuvo los mejores resultados? Explique por qué cree que fue así."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mOGoiDhGe4MH"
   },
   "source": [
    "## Performance de los modelos\n",
    "\n",
    "En el caso anterior, para medir la cantidad de artículos clasificados correctamente se utilizó el mismo subconjunto del dataset que se utilizó para entrenar.\n",
    "\n",
    "Esta medida no es una medida del todo útil, ya que lo que interesa de un clasificador es su capacidad de clasificación de datos que no fueron utilizados para entrenar. Es por eso que se pide, para el clasificador entrenado con el subconjunto de training, cual es el porcentaje de artículos del subconjunto de testing clasificados correctamente. Comparar con el porcentaje anterior y explicar las diferencias.\n",
    "\n",
    "Finalmente deben observar las diferencias y extraer conclusiones en base al accuracy obtenido, el preprocesamiento y vectorización utilizado y el modelo, para cada combinación de posibilidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fsnhfAiwg-A-"
   },
   "outputs": [],
   "source": [
    "#Loading the data set - training data.\n",
    "twenty_test = get_20newsgroup(subset='test', metadata=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_filtered_joined_articles(name='train-nometadata', \n",
    "                                        articles=twenty_train.data,\n",
    "                                        filts=best_hyper.filts)\n",
    "test_data = get_filtered_joined_articles(name='test-nometadata', \n",
    "                                        articles=twenty_test.data,\n",
    "                                        filts=best_hyper.filts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UpphW8cAVcrX"
   },
   "outputs": [],
   "source": [
    "count_vect = eval(best_hyper.vectorizer)(max_df=best_hyper.max_df, min_df=best_hyper.min_df)\n",
    "raw_data_train = count_vect.fit_transform(train_data) \n",
    "raw_data_test = count_vect.transform(test_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6801646309081253"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = eval(best_hyper.model)()\n",
    "clf.fit(raw_data_train, twenty_train.target)\n",
    "score = clf.score(raw_data_test, twenty_test.target)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wondering', 'enlighten', 'car', 'saw', 'door', 'sports', 'looked', 'late', '60s', 'early', '70s', 'bricklin', 'doors', 'small', 'addition', 'front', 'bumper', 'separate', 'rest', 'body', 'tellme', 'model', 'engine', 'specs', 'production', 'history', 'whatever', 'info', 'funky', 'fair', 'brave', 'souls', 'upgraded', 'si', 'clock', 'oscillator', 'shared', 'experiences', 'poll', 'send', 'brief', 'message', 'detailing', 'procedure', 'top', 'speed', 'attained', 'cpu', 'rated', 'add', 'cards', 'adapters', 'heat', 'sinks', 'hour', 'usage', 'per', 'floppy', 'disk', 'functionality', '800', 'floppies', 'especially', 'requested', 'summarizing', 'next', 'days', 'network', 'knowledge', 'base', 'upgrade', 'answered', 'folks', 'mac', 'plus', 'finally', 'gave', 'ghost', 'weekend', 'starting', 'life', '512k', '1985', 'sooo', 'market', 'machine', 'sooner', 'intended', 'picking', 'powerbook', '160', '180', 'bunch', 'questions', 'hopefully', 'somebody', 'answer', 'anybody', 'dirt', 'round', 'introductions', 'expected', 'heard', '185c', 'supposed', 'appearence', 'summer', 'anymore', 'access', 'macleak', 'rumors', 'price', 'drops', 'line', 'ones', 'duo', 'went', 'recently', 'impression', 'display', 'swing', '80mb', '120', 'feel', 'yea', 'looks', 'store', 'wow', 'solicit', 'opinions', 'worth', 'taking', 'size', 'money', 'hit', 'active', 'realize', 'subjective', 'played', 'machines', 'computer', 'breifly', 'figured', 'uses', 'daily', 'prove', 'helpful', 'hellcats', 'perform', 'advance', 'email', 'summary', 'news', 'reading', 'premium', 'finals', 'corner', 'tom', 'willis', 'twillis', 'ecn', 'purdue', 'electrical', 'engineering', 'weitek', 'address', 'phone', 'chip', 'article', 'c5owcb', 'n3p', 'std', 'tombaker', 'baker', 'understanding', 'errors', 'basically', 'known', 'bugs', 'warning', 'software', 'checked', 'values', 'yet', 'aren', 'till', 'launch', 'suchlike', 'fix', 'code', 'possibly', 'introduce', 'crew', 'ok', '213', 'liftoff', 'ignore', 'term', 'rigidly', 'defined']\n",
      "Palabras totales: 200\n",
      "Palabras distintas: 200\n"
     ]
    }
   ],
   "source": [
    "print_list(list(count_vect.vocabulary_.keys())[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_df=0.05)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(best_hyper.vectorizer)(max_df=best_hyper.max_df, min_df=best_hyper.min_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iOf-BcvpeqKp"
   },
   "source": [
    "Preguntas\n",
    "\n",
    "- El accuracy en el dataset de test es mayor o menor que en train? Explique por qué."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ejercicio_Clasificación_Texto.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
