{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ejercicio_Clasificación_Texto.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rocioparra/redes-neuronales/blob/master/03-clasificacion-texto/Ejercicio_Clasificaci%C3%B3n_Texto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8mBlkilaNC2M",
        "colab": {}
      },
      "source": [
        "# Estos dos comandos evitan que haya que hacer reload cada vez que se modifica un paquete\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4XI-bramQQLk"
      },
      "source": [
        "# Ejercicio de clasificación de texto\n",
        "\n",
        "Naive Bayes es una técnica estadística que consiste en repetir el método anterior en problemas cuyos sucesos no son independientes, pero suponiendo independencia.\n",
        "A lo largo de este trabajo desarrollarán un modelo de Naive Bayes para el problema de clasificación de artículos periodístios.En este caso podemos estimar la probabilidad de ocurrencia de cada palabra según la categoría a la que pertenece el artículo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zJ7IlPHXEDEJ"
      },
      "source": [
        "## Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z8adA_beEXG7"
      },
      "source": [
        "El primer paso es obtener el dataset que vamos a utilizar. El dataset a utilizar es el de TwentyNewsGroup(TNG) que está disponible en sklearn.\n",
        "\n",
        "Se puede encontrar más información del dataset en la documentación de scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmSiAFf7L69o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from helper import autosave\n",
        "\n",
        "# autosave asegura que se guarden los datos en un archivo\n",
        "@autosave('twenty-md={metadata}-{subset}')\n",
        "def get_20newsgroup(subset='train', metadata=False):\n",
        "    if metadata:\n",
        "        return fetch_20newsgroups(subset=subset, shuffle=True)\n",
        "    else:\n",
        "        # de acuerdo a recomendacion de sklearn, no considerar metadata\n",
        "        # para obtener resultados mas representativos\n",
        "        return fetch_20newsgroups(subset=subset, remove=('headers', 'footers', 'quotes'), shuffle=True)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AHaexjmQNQej",
        "colab": {}
      },
      "source": [
        "#Loading the data set - training data.\n",
        "\n",
        "twenty_train = get_20newsgroup(subset='train', metadata=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u65YieUTEgR3"
      },
      "source": [
        "El siguiente paso es analizar el contenido del dataset, como por ejemplo la cantidad de artículos, la cantidad de clases, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sx2_w9hbS9-D"
      },
      "source": [
        "### Preguntas\n",
        "\n",
        "1) ¿Cuántos articulos tiene el dataset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dmBRcNNbRvVu",
        "colab": {}
      },
      "source": [
        "len(twenty_train.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwXoKjPbL69y",
        "colab_type": "text"
      },
      "source": [
        "2) ¿Cuántas clases tiene el dataset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMdZlCIzL69y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(twenty_train.target_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNtJzyqfL691",
        "colab_type": "text"
      },
      "source": [
        "3) ¿Es un dataset balanceado?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQdBhjEAL691",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "_, counts = np.unique(twenty_train.target, return_counts=True)\n",
        "if len(set(counts)) == 1:\n",
        "    print('El dataset está balanceado')\n",
        "else:    \n",
        "    print('El dataset no está balanceado')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SIgt_PmL694",
        "colab_type": "text"
      },
      "source": [
        "4) ¿Cuál es la probabilidad a priori de la clase 5? A que corresponde esta clase?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESb00vAIL694",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "priori5 = counts[5]/sum(counts)\n",
        "print(f'La clase 5 ({twenty_train.target_names[5]}) tiene probabilidad a priori {priori5:3}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ec_R4C_L696",
        "colab_type": "text"
      },
      "source": [
        "5) ¿Cuál es la clase con mayor probabilidad a priori?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjA_Dl3XL697",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_class = np.argmax(counts)\n",
        "print(f'La clase {max_class} ({twenty_train.target_names[max_class]}) tiene la máxima probabilidad a priori, {counts[max_class]/sum(counts):.2}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "84hQvk-UHX1V"
      },
      "source": [
        "## Preprocesamiento\n",
        "\n",
        "Para facilitar la comprensión de los algoritmos de preprocesamiento, se aplican primero a un solo artículo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l0zxZSOsFXpl"
      },
      "source": [
        "\n",
        "Mas info en:\n",
        "http://text-processing.com/demo/stem/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFohGXubL69-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from helper import print_list\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "article = twenty_train.data[0]\n",
        "article"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-geItZPyUWEO"
      },
      "source": [
        "- **Tokenization (nltk):**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKGHy7YRL6-B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tok = word_tokenize(article)\n",
        "print_list(tok)\n",
        "# print feo porque me ponia enters y quedaba muy largo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pksduBGKFMxt"
      },
      "source": [
        "- **Lemmatization (nltk):**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Kv4WDZIL6-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "lem=[lemmatizer.lemmatize(x, pos='v') for x in tok]\n",
        "print_list(lem)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-zxqqFYAFOsN"
      },
      "source": [
        "- **Stop Words (nltk):**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EPZnKNRL6-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop = [x for x in lem if x not in stopwords.words('english')]\n",
        "print_list(stop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVGV-_0sL6-I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(stopwords.words('english')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UjKQKWLGFQRl"
      },
      "source": [
        "- **Stemming (nltk):**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fn3_J-hL6-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stem = [stemmer.stem(x) for x in stop]\n",
        "print_list(stem)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HNfMRbqmFSIR"
      },
      "source": [
        "- **Filtrado de palabras:**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6m-gGPaL6-N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alpha = [x for x in stem if x.isalpha()]\n",
        "print_list(alpha)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yOzGVenvIESr"
      },
      "source": [
        "### Preprocesamiento completo\n",
        "\n",
        "Utilizar o no cada uno de los métodos vistos es una decisión que dependerá del caso particular de aplicación. Para este ejercicio vamos a considerar las siguientes combinaciones:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5y31VaboL6-Q",
        "colab_type": "text"
      },
      "source": [
        "- Tokenización\n",
        "- Tokenización, Lematización, Stemming.\n",
        "- Tokenización, Stop Words.\n",
        "- Tokenización, Lematización, Stop Words, Stemming.\n",
        "- Tokenización, Lematización, Stop Words, Stemming, Filtrado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ay3bgiGKL6-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def filter_article(article, filts):\n",
        "    filts = filts.split()\n",
        "\n",
        "    if 'lem' in filts:\n",
        "        article = [lemmatizer.lemmatize(x,pos='v') for x in article]\n",
        "\n",
        "    if 'stop' in filts:\n",
        "        article = [x for x in article if x not in stopwords.words('english')]\n",
        "\n",
        "    if 'stem' in filts:\n",
        "        article = [stemmer.stem(x) for x in article]\n",
        "\n",
        "    if 'filt' in filts:\n",
        "        article = [x for x in article if x.isalpha()]\n",
        "    \n",
        "    return article\n",
        "\n",
        "\n",
        "@autosave(fmt='{name}-{filts}')\n",
        "def filter_articles(name, articles, filts):\n",
        "    \n",
        "    filtered_articles = []\n",
        "    for data in articles:\n",
        "        tok = word_tokenize(data)\n",
        "        filtered_articles.append(filter_article(tok, filts))\n",
        "            \n",
        "    return filtered_articles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mmFlpfAL6-T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ans_fmt = \"\"\"Preprocesamiento: {preproc}\n",
        "Longitud del vocabulario: {vocab_len}\n",
        "\"\"\"\n",
        "preprocessing = ['tok', 'tok lem stem', 'tok stop', 'tok lem stop stem', 'tok lem stop stem filt']\n",
        "\n",
        "    \n",
        "for preproc in preprocessing:\n",
        "    filtered_articles = filter_articles('train-nometadata', twenty_train.data, preproc)\n",
        "    vocab = set([word for article in filtered_articles for word in article])\n",
        "    print(ans_fmt.format(preproc=preproc, vocab_len=len(vocab)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wP3n8ZwFc7_D"
      },
      "source": [
        "### Preguntas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Xn3CaOKL6-X",
        "colab_type": "text"
      },
      "source": [
        "- **Cómo cambia el tamaño del vocabulario al agregar Lematización y Stemming?**\n",
        "\n",
        "Sólo con tokenización: 161698 palabras\n",
        "\n",
        "Con lematización y stemming: 126903 palabras (21.5% menor, 34795 palabras menos).\n",
        "\n",
        "El vocabulario se reduce no porque la cantidad total de palabras sea menor, sino porque palabras que originalmente eran distintas ahora son iguales (por ejemplo, \"is\" y \"was\" se convierten ambas en \"be\").\n",
        "\n",
        "La reducción del 20% sugiere que de cada 5 palabras, dos son dos \"versiones\" de la misma, lo cual es razonable si se tiene en cuenta que es muy común usar una misma palabra en singular y plural en un mismo contexto (\"car\" / \"cars\"), adverbios y adjetivos con la misma raíz (\"real\" / \"really\"), pronombres en distintos casos (\"I\" / \"me\"), etcétera (en español esto probablemente sería incluso más pronunciado, al haber mayor cantidad de declinaciones verbales distintas, y tener géneros para adjetivos y artículos)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGVqDY1rL6-Y",
        "colab_type": "text"
      },
      "source": [
        "- **Cómo cambia el tamaño del vocabulario al Stop Words?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_soRRxO9L6-Y",
        "colab_type": "text"
      },
      "source": [
        "Sólo con tokenización: 161698 palabras\n",
        "\n",
        "Con stop words: 161533 palabras (0.1% menor, 165 palabras menos)\n",
        "\n",
        "El vocabulario se reduce porque se remueven las palabras contenidas en el conjunto de stop words. Es razonable entonces que la reducción del vocabulario sea menor de este caso, ya que como máximo se podrán remover tantas palabras como haya en la lista de stop words (en este caso, 179)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUjaXhRLL6-Z",
        "colab_type": "text"
      },
      "source": [
        "- **Analice muy brevemente ventajas y desventajas del tamaño del dataset en cada caso.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsBFRFbpL6-Z",
        "colab_type": "text"
      },
      "source": [
        "Para el caso de stopwords, es útil porque es razonable pensar que ese conjunto de palabras estará presente en todos los artículos, y por lo tanto no aportará demasiada información. Por ejemplo, una palabra como \"car\", \"player\" o \"God\" nos da más información sobre de qué se está hablando que \"in\", \"no\" o \"is\".\n",
        "\n",
        "Sin embargo, también pueden imaginarse casos donde esto no sea cierto. Por ejemplo, si una clase se caracteriza por narraciones en primera persona, mientras que los demás suelen ser más impersonales, se estaría ignorando la información que aportan palabras como \"I\", \"me\", etc. \n",
        "\n",
        "En cuanto a la lematización y stemming, puede hacerse un análisis muy similar. En muchos casos, es muy útil combinar las probabilidades de palabras similares: en un artículo de deportes, las palabras \"match\" y \"matches\" apuntan a un contexto similar. Lo mismo puede decirse en cuanto a el tiempo de los verbos: no es particularmente relevante para determinar si se está hablando de deportes, ya que podría estarse especulando sobre o anunciando eventos que sucederán en el futuro (\"if they win\"), o relatando eventos del pasado (\"after they won\"). Por lo tanto, tratar ambos casos como idénticos podría ser beneficioso.\n",
        "\n",
        "Sin embargo, nuevamente pueden pensarse casos donde esto no será cierto: puede que una clase se caracterice por hablar sobre eventos del pasado, mientras que otra se dedique más a explicaciones en presente, y ese matiz se perdería con lematización y stemming."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "c-R-LPRoIV0E"
      },
      "source": [
        "## Vectorización de texto\n",
        "\n",
        "- **Obtención del vocabulario y obtención de la probabilidad**\n",
        "\n",
        "Como se vió en clase, los vectorizadores cuentan con dos parámetros de ajuste.\n",
        "\n",
        "- max_df: le asignamos una maxima frecuencia de aparición, eliminando las palabras comunes que no aportan información.\n",
        "\n",
        "- min_df: le asignamos la minima cantidad de veces que tiene que aparecer una palabra.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G7ob-IgsMRS7"
      },
      "source": [
        "## Entrenamiento del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a4bVz9QwMupE"
      },
      "source": [
        "Primero deben separar correctamente el dataset para hacer validación del modelo.\n",
        "Y luego deben entrenar el modelo de NaiveBayes con el dataset de train.\n",
        "\n",
        "Deben utilizar un modelo de NaiveBayes Multinomial y de Bernoulli. Ambos modelos estan disponibles en sklearn.\n",
        "\n",
        "Finalmente comprobar el accuracy en train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81G78exKL6-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@autosave(fmt='{name}-joined-{filts}')\n",
        "def get_filtered_joined_articles(name, articles, filts):\n",
        "    articles = filter_articles(name, articles, filts)\n",
        "    for i in range(len(articles)):\n",
        "        articles[i] = ' '.join(articles[i])\n",
        "        \n",
        "    return articles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C74USIEyS3B7",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
        "import pandas as pd\n",
        "import os.path\n",
        "\n",
        "\n",
        "OUT_FILE = 'out-nometadata4.csv'\n",
        "\n",
        "vectorizers = [CountVectorizer, TfidfVectorizer]\n",
        "classifiers = [MultinomialNB, BernoulliNB]\n",
        "max_dfs = [.05, 0.1, 0.25, 0.3, 0.5, 0.75, 0.8, 0.95]\n",
        "min_dfs = [1, 2, 5, 10]\n",
        "\n",
        "results = []\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "total_its = len(vectorizers)*len(classifiers)*len(min_dfs)*len(max_dfs)*len(preprocessing)\n",
        "i = 1\n",
        "\n",
        "if os.path.isfile(OUT_FILE):\n",
        "    results = pd.read_csv(OUT_FILE)\n",
        "else:\n",
        "    results = []\n",
        "\n",
        "    for filts in preprocessing:\n",
        "        data = get_filtered_joined_articles(name='train-nometadata', filts=filts) \n",
        "        \n",
        "        for max_df in max_dfs:\n",
        "            for min_df in min_dfs:\n",
        "                \n",
        "                for Vectorizer in vectorizers:\n",
        "                    count_vect = Vectorizer(max_df=max_df, min_df=min_df)\n",
        "                    raw_data = count_vect.fit_transform(data) \n",
        "\n",
        "                    for Classifier in classifiers:\n",
        "                        print(f'Computing: F={filts}, M={max_df}, m={min_df}, V={Vectorizer.__name__}, C={Classifier.__name__} ({i}/{total_its})...')\n",
        "                        i += 1\n",
        "                        \n",
        "                        clf = Classifier()\n",
        "                        clf.fit(raw_data, twenty_train.target)\n",
        "                        score = clf.score(raw_data, twenty_train.target)\n",
        "\n",
        "                        results.append({\n",
        "                            'max_df': max_df,\n",
        "                            'min_df': min_df,\n",
        "                            'filts': filts,\n",
        "                            'vectorizer': Vectorizer.__name__,\n",
        "                            'model': Classifier.__name__,\n",
        "                            'score': score\n",
        "                        })\n",
        "                        \n",
        "                        print(f'Score: {score:.5%}')\n",
        "                        print(50 * '-')\n",
        "    results = pd.DataFrame(results)\n",
        "    results.to_csv(OUT_FILE, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GPPo4Fr-eSON"
      },
      "source": [
        "### Preguntas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKoJGowhcufh",
        "colab_type": "text"
      },
      "source": [
        "- **¿Con que modelo obtuvo los mejores resultados? Explique por qué cree que fue así.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nafm4bJcL6-v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_hyper = best_hypers.iloc[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3iqIoNyTTYVc",
        "colab": {}
      },
      "source": [
        "best = results['score'].max()\n",
        "best_filt = results['score']==best\n",
        "best_hypers = results[best_filt]\n",
        "best_hypers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJmsffK4ehiK",
        "colab_type": "text"
      },
      "source": [
        "Los mejores resultados se obtenieron utilizando:\n",
        "- clasificador multinomial\n",
        "- vectorizador TF-IDF, con min_df=1 y max_df=0.05\n",
        "- preprocesamiento: únicamente tokenización\n",
        "\n",
        "No se disminuyó más el valor de max_df para evitar overfitting (quedarse sólo con palabras que estén en uno o dos artículos, y por lo tanto poder determinar la clase casi determinísticamente para el set de train, pero probablemente obteniendo un modelo no muy útil para analizar otros sets de datos).\n",
        "\n",
        "El valor de max_df=0.05 sugiere que se eliminan del set palabras que sean comunes en más de una clase (si bien el set no está exactamente balanceado, se vio previamente que la máxima probabilidad es de 0.053, y si fuese 0.05 todas las clases tendrían la misma). Con min_df=1 no se descarta ninguna palabra que aparezca en al menos un artículo, y utilizar solo tok hace que las palabras queden tal como estaban. Considerando la naturaleza especializada de las clases, y lo distintas que son entre sí (ateísmo, hardware IBM, MS Windows,  armas, política en el medio oriente, espacial, cripto...), probablemente lo que esté sucediendo es que se conserva sólo un vocabulario lo suficientemente especializado como para detectar con la mayor certeza posible a cuál de las 20 clases pertenece.\n",
        "\n",
        "Esto es consistente con que los mejores resultados se obtengan con TF-IDF, ya que de esta manera se está penalizando a palabras que son comunes entre muchos artículos, nuevamente haciendo foco en vocabulario especializado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzO8qZS3L6-f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results.sort_values(by=['score'], ascending=False, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nV35IRbL6-p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAJqBBkOL6-h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results[results.model=='MultinomialNB'].sort_values(by='score', ascending=True).head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUTwVo0NL6-l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results[results.model=='BernoulliNB'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHjTNR_yc2YC",
        "colab_type": "text"
      },
      "source": [
        "Se obtuvieron mejores resultados con el modelo multinomial. De las 640 combinaciones de hiperparámetros testeadas, absolutamente todas las pruebas con el modelo de Bernoulli arrojaron peores resultados que el peor scoring obtenido para multinomial (0.6828 para el mejor caso de Bernoulli, y 0.7235 para el peor de multinomial).\n",
        "\n",
        "Este resultado es el esperado, ya que el modelo multinomial representa más fielmente los datos de este problema en particular: la ocurrencia de una palabra en un artículo aporta mucha menos información si se considera binariamente (palabra está presente / no está presente en el artículo). Con el modelo de Bernoulli, en este caso, se está perdiendo información."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mOGoiDhGe4MH"
      },
      "source": [
        "## Performance de los modelos\n",
        "\n",
        "En el caso anterior, para medir la cantidad de artículos clasificados correctamente se utilizó el mismo subconjunto del dataset que se utilizó para entrenar.\n",
        "\n",
        "Esta medida no es una medida del todo útil, ya que lo que interesa de un clasificador es su capacidad de clasificación de datos que no fueron utilizados para entrenar. Es por eso que se pide, para el clasificador entrenado con el subconjunto de training, cual es el porcentaje de artículos del subconjunto de testing clasificados correctamente. Comparar con el porcentaje anterior y explicar las diferencias.\n",
        "\n",
        "Finalmente deben observar las diferencias y extraer conclusiones en base al accuracy obtenido, el preprocesamiento y vectorización utilizado y el modelo, para cada combinación de posibilidades."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fsnhfAiwg-A-",
        "colab": {}
      },
      "source": [
        "#Loading the data set - training data.\n",
        "twenty_test = get_20newsgroup(subset='test', metadata=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qx3hZfcRL6-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = get_filtered_joined_articles(name='train-nometadata', \n",
        "                                        articles=twenty_train.data,\n",
        "                                        filts=best_hyper.filts)\n",
        "test_data = get_filtered_joined_articles(name='test-nometadata', \n",
        "                                        articles=twenty_test.data,\n",
        "                                        filts=best_hyper.filts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UpphW8cAVcrX",
        "colab": {}
      },
      "source": [
        "count_vect = eval(best_hyper.vectorizer)(max_df=best_hyper.max_df, min_df=best_hyper.min_df)\n",
        "raw_data_train = count_vect.fit_transform(train_data) \n",
        "raw_data_test = count_vect.transform(test_data) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hacL4g4UL6-9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf = eval(best_hyper.model)()\n",
        "clf.fit(raw_data_train, twenty_train.target)\n",
        "score = clf.score(raw_data_test, twenty_test.target)\n",
        "score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iOf-BcvpeqKp"
      },
      "source": [
        "### Preguntas\n",
        "\n",
        "- **El accuracy en el dataset de test es mayor o menor que en train? Explique por qué.**\n",
        "\n",
        "La accuracy es menor en test que en train porque los parámetros del modelo son los estimados a partir de train, y por lo tanto está garantizado que dado un set de hiperparámetros y un modelo dado, será la mejor representación de los datos posible. Esto no es cierto con test - podría darse que en algún caso en particular, la información de test se ajuste a la obtenida a partir de train, incluso mejor que el mismo train, pero esto no es una garantía (y no es lo que se espera, ni en lo que en la mayoría de los casos ocurre)."
      ]
    }
  ]
}